{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the blocks below only for the first time\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sh env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained models (may take a minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import shutil\n",
    "# for zip file\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# url = \"https://onedrive.live.com/?cid=5435770760f02d2f&id=5435770760F02D2F%211152&authkey=!AD6jcLtyet8Y3o4\"\n",
    "    \n",
    "# r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "# open('cpvton-plus-checkpoints.zip', 'wb').write(r.content)\n",
    "\n",
    "def download(url, filename, cookies=None):\n",
    "    with open(filename, 'wb') as f:\n",
    "        response = requests.get(url, stream=True, cookies=cookies)\n",
    "        total = response.headers.get('content-length')\n",
    "\n",
    "        if total is None:\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            downloaded = 0\n",
    "            total = int(total)\n",
    "            for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
    "                downloaded += len(data)\n",
    "                f.write(data)\n",
    "                completed = int(50*downloaded/total)\n",
    "                sys.stdout.write('\\r[{}{}]'.format(\n",
    "                    'â–ˆ' * completed, '.' * (50-completed)))\n",
    "                sys.stdout.flush()\n",
    "    sys.stdout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Downloading data...\n",
      "\n",
      "[^] Download complete\n"
     ]
    }
   ],
   "source": [
    "drive_request = requests.get(\n",
    "    'https://drive.google.com/uc?export=download&confirm=CONFIRM&id=199dwgFGlumJ95MAGNbLnYtbVKA2w6kdc')\n",
    "confirm_page = drive_request.text\n",
    "confirmation_code = re.findall('confirm=(.{4})', confirm_page)[0]\n",
    "\n",
    "print('[*] Downloading data...')\n",
    "download('https://drive.google.com/uc?export=download&confirm=CONFIRM&id=199dwgFGlumJ95MAGNbLnYtbVKA2w6kdc'.replace(\n",
    "    'CONFIRM', confirmation_code), 'cpvton-plus-checkpoints.zip', cookies=drive_request.cookies)\n",
    "print('[^] Download complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excuate the block below once you see a 'cpvton-plus-checkpoints.zip' in the storage directory\n",
    "#### Extract the checkpoint to 'storage/checkpoints-cpvton-plus' directory and delete the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('checkpoints-cpvton-plus'):\n",
    "    os.makedirs('checkpoints-cpvton-plus')\n",
    "checkpoint = zipfile.ZipFile(\"cpvton-plus-checkpoints.zip\")\n",
    "checkpoint.extractall(path='checkpoints-cpvton-plus')\n",
    "shutil.move('checkpoints-cpvton-plus/checkpoints/GMM/', 'checkpoints-cpvton-plus/GMM/')\n",
    "shutil.move('checkpoints-cpvton-plus/checkpoints/TOM/', 'checkpoints-cpvton-plus/TOM/')\n",
    "os.rmdir('checkpoints-cpvton-plus/checkpoints')\n",
    "os.remove('cpvton-plus-checkpoints.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir('checkpoints-cpvton-plus/checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the whole VITON datasets and relocate the data (may take 20 minutes or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "drive_request = requests.get(\n",
    "    'https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo')\n",
    "confirm_page = drive_request.text\n",
    "confirmation_code = re.findall('confirm=(.{4})', confirm_page)[0]\n",
    "\n",
    "print('[*] Downloading data...')\n",
    "download('https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo'.replace(\n",
    "    'CONFIRM', confirmation_code), 'data/viton_resize.tar.gz', cookies=drive_request.cookies)\n",
    "\n",
    "tarfile.open(\"data/viton_resize.tar.gz\").extractall(path='data/')\n",
    "\n",
    "shutil.move('data/viton_resize/test/', 'data/test/')\n",
    "shutil.move('data/viton_resize/train/', 'data/train/')\n",
    "\n",
    "os.rmdir('data/viton_resize/')\n",
    "os.remove('data/viton_resize.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the blocks above only for the first time\n",
    "### Run blocks below for test stage\n",
    "#### Change directory to root \n",
    "##### Run blocks below once you see train and test folders in 'storage/data' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working in the right location\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../notebooks')\n",
    "if (os. getcwd() == '/notebooks'):\n",
    "    print('Now working in the right location')\n",
    "else:\n",
    "    print('ERROE! Current directory is:', os. getcwd())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build GMM results to wrap the clothes based on test models' poses (I changed the python scripts directly so below commends run by bash)\n",
    "### Each lines may take a long time to run\n",
    "#### You can check the results after the procedure finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Stage \n",
    "#### Generate stage one sagementation results with neck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python dataset_neck_skin_correction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python body_binary_masking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIHP_PGN pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_request = requests.get(\n",
    "    'https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1Mqpse5Gen4V4403wFEpv3w3JAsWw2uhk')\n",
    "confirm_page = drive_request.text\n",
    "confirmation_code = re.findall('confirm=(.{4})', confirm_page)[0]\n",
    "\n",
    "print('[*] Downloading data...')\n",
    "download('https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1Mqpse5Gen4V4403wFEpv3w3JAsWw2uhk'.replace(\n",
    "    'CONFIRM', confirmation_code), 'CIHP_PGN-checkpoints.zip', cookies=drive_request.cookies)\n",
    "print('[^] Download complete')\n",
    "if not os.path.exists('checkpoints-CIHP_PGN'):\n",
    "    os.makedirs('checkpoints-CIHP_PGN')\n",
    "checkpoint = zipfile.ZipFile(\"CIHP_PGN-checkpoints.zip\")\n",
    "checkpoint.extractall(path='checkpoints-CIHP_PGN')\n",
    "\n",
    "os.remove('CIHP_PGN-checkpoints.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode test --data_list test_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/checkpoints/GMM/gmm_final.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r storage/result-cpvton-plus/GMM/test/warp-cloth storage/data/test\n",
    "cp -r storage/result-cpvton-plus/GMM/test/warp-mask storage/data/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python test.py --name TOM --dataroot storage/data --stage TOM --workers 4 --datamode test --data_list test_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/TOM/tom_final.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv -r result storage/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### The results for cp-vton-plus now can be checked in 'storage/result/TOM/test/try-on'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python train.py --name GMM --dataroot storage/data --stage GMM --workers 4 --decay_step 35000 --keep_step 35000 --data_list train_pairs.txt --checkpoint_dir storage/checkpoints-cpvton-plus/ --save_count 5000 --shuffle --tensorboard_dir storage/tensorboard-CPVTON-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode train --data_list train_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/GMM/gmm_raw.pth --result_dir storage/result-cpvton-plus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r storage/result-cpvton-plus/GMM/train/warp-cloth storage/data/train\n",
    "cp -r storage/result-cpvton-plus/GMM/train/warp-mask storage/data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard  --logdir= storage/tensorboard-CPVTON-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --name TOM --dataroot storage/data --stage TOM --workers 4 --decay_step 35000 --keep_step 35000 --data_list train_pairs.txt --checkpoint_dir storage/checkpoints-cpvton-plus/ --save_count 5000 --shuffle --tensorboard_dir storage/tensorboard-CPVTON-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jen added: Trace model\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.load('storage/checkpoints-cpvton-plus/TOM/tom_raw.pth')\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARK: Please try the tracing method here\n",
    "\n",
    "example_input = torch.rand(64, 26, 192,256)\n",
    "traced_model = torch.jit.trace(torch_model,example_input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf = zipfile.ZipFile('train-opt.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('storage/data/train-opt', zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python cloth_binary_masking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --name GMM --dataroot storage/data --datamode train-opt --stage GMM --workers 4 --decay_step 35000 --keep_step 35000 --data_list train_pairs.txt --checkpoint_dir storage/checkpoints-cpvton-plus/ --save_count 5000 --shuffle --tensorboard_dir storage/tensorboard-CPVTON-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode train-opt --data_list train_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/GMM/gmm_opt.pth --result_dir storage/result-cpvton-plus-opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r storage/result-cpvton-plus-opt/GMM/train-opt/warp-cloth storage/data/train-opt\n",
    "cp -r storage/result-cpvton-plus-opt/GMM/train-opt/warp-mask storage/data/train-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode test-end2end --data_list train_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/GMM/gmm_raw.pth --result_dir storage/result-cpvton-plus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new mission: TRAIN TOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --name TOM --dataroot storage/data --stage TOM --workers 4 --datamode train-opt --data_list train_pairs.txt --checkpoint_dir storage/checkpoints-cpvton-plus/ --save_count 5000 --shuffle --tensorboard_dir storage/tensorboard-CPVTON-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Downloading data...\n",
      "\n",
      "[^] Download complete\n"
     ]
    }
   ],
   "source": [
    "drive_request = requests.get(\n",
    "    'https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1B-__YtFz9S8zXWiwF4WEoFh_213dHaaC')\n",
    "confirm_page = drive_request.text\n",
    "confirmation_code = re.findall('confirm=(.{4})', confirm_page)[0]\n",
    "\n",
    "print('[*] Downloading data...')\n",
    "download('https://drive.google.com/uc?export=download&confirm=CONFIRM&id=1B-__YtFz9S8zXWiwF4WEoFh_213dHaaC'.replace(\n",
    "    'CONFIRM', confirmation_code), 'joint-points-test.zip', cookies=drive_request.cookies)\n",
    "print('[^] Download complete')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = zipfile.ZipFile(\"joint-points-test.zip\")\n",
    "checkpoint.extractall(path='storage/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python segamentation/simple_extractor.py --dataset 'lip' --model-restore storage/checkpoints-humanParsing/exp-schp-201908261155-lip.pth --input-dir storage/data/test-end2end/image --output-dir storage/data/test-end2end/image-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode test-end2end --data_list test_end2end_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/GMM/gmm_opt.pth --result_dir storage/result-cpvton-plus-testend2end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TOM by end2end dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working in the right location\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, sys, datetime, signal, shutil\n",
    "os.chdir('../notebooks')\n",
    "if (os. getcwd() == '/notebooks'):\n",
    "    print('Now working in the right location')\n",
    "else:\n",
    "    print('ERROE! Current directory is:', os. getcwd())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_clear = \"rm -r storage/data/test-end2end\"\n",
    "subprocess.call(cmd_clear, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cloth image pair to 'storage/data/test_end2end_pair.txt'\n",
    "#### image and cloth must be exist in storage/data/test/image and storage/data/test/cloth\n",
    "#### e.g. 000010_0.jpg 000001_1.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothes and models you choose should be located in storage/data/test-end2end, please check (reload folder may take a few second)!!!\n"
     ]
    }
   ],
   "source": [
    "dataroot = 'storage/data/'\n",
    "data_list = 'test_end2end_pairs.txt'\n",
    "cloth_src = 'storage/data/test/cloth'\n",
    "image_src = 'storage/data/test/image'\n",
    "cloth_dir = 'storage/data/test-end2end/cloth'\n",
    "image_dir = 'storage/data/test-end2end/image'\n",
    "pose_src = 'storage/data/test-opt/pose'\n",
    "pose_dir = 'storage/data/test-end2end/pose'\n",
    "im_names = []\n",
    "c_names = []\n",
    "if not os.path.exists(cloth_dir):\n",
    "    os.makedirs(cloth_dir)\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "if not os.path.exists(pose_dir):\n",
    "    os.makedirs(pose_dir)\n",
    "with open(os.path.join(dataroot, data_list), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                im_name, c_name = line.strip().split()\n",
    "                im_names.append(im_name)\n",
    "                c_names.append(c_name)\n",
    "def copy_files_dir(files_list, src, dest):\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        if file_name in files_list:\n",
    "            shutil.copy(os.path.join(src, file_name), dest)\n",
    "poses = os.listdir(pose_src)\n",
    "for pose in poses:\n",
    "    image_name = pose.split('_')[0]+'_0.jpg'\n",
    "    if image_name in im_names:\n",
    "        shutil.copy(os.path.join(pose_src, pose), pose_dir)\n",
    "    \n",
    "copy_files_dir(im_names, image_src, image_dir)\n",
    "copy_files_dir(c_names, cloth_src, cloth_dir)\n",
    "print(\"clothes and models you choose should be located in storage/data/test-end2end, please check (reload folder may take a few second)!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd1 = \"python segamentation/simple_extractor.py --dataset 'lip' --model-restore storage/checkpoints-humanParsing/exp-schp-201908261155-lip.pth --input-dir storage/data/test-end2end/image --output-dir storage/data/test-end2end/image-parse\"\n",
    "subprocess.call(cmd1, shell=True)\n",
    "\n",
    "# add neck to segamentation\n",
    "cmd2 = \"python dataset_neck_skin_correction.py\"\n",
    "subprocess.call(cmd2, shell=True)\n",
    "\n",
    "# binary masking\n",
    "cmd3 = \"python body_binary_masking.py\"\n",
    "subprocess.call(cmd3, shell=True)\n",
    "# clothes mask\n",
    "cmd4 = \"python cloth_binary_masking.py\"\n",
    "subprocess.call(cmd4, shell=True)\n",
    "#GMM\n",
    "cmd5 = \"python test.py --name GMM --dataroot storage/data --stage GMM --workers 4 --datamode test-end2end --data_list test_end2end_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/GMM/gmm_opt.pth --result_dir storage/result-cpvton-plus-opt\"\n",
    "subprocess.call(cmd5, shell=True)\n",
    "#Move GMM results to working directory\n",
    "cmd6 = \"mv -v storage/result-cpvton-plus-opt/GMM/test-end2end/warp-cloth storage/data/test-end2end\"\n",
    "subprocess.call(cmd6, shell=True)\n",
    "cmd7 = \"mv -v storage/result-cpvton-plus-opt/GMM/test-end2end/warp-mask storage/data/test-end2end\"\n",
    "subprocess.call(cmd7, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's time to test cp-vton-plus-opt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd8 = \"python test.py --name TOM --dataroot storage/data --stage TOM --workers 4 --datamode test-end2end --data_list test_end2end_pairs.txt --checkpoint storage/checkpoints-cpvton-plus/TOM/tom_opt.pth --result_dir storage/result-cpvton-plus-testend2end\"\n",
    "subprocess.call(cmd8, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results saved in storage/result-cpvton-plus-testend2end/TOM/test-end2end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
